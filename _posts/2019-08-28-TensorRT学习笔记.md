---
toc: true
layout: post
description: A example of how to use TensorRT.
categories: [markdown]
title: TensorRT学习笔记
---
TensorRT是英伟达推出的高性能深度学习推断平台。它包含了一个深度学习推断优化器和一个为深度学习推断应用提供的低延迟、大吞吐运行环境。在推断时能比CPU快40倍左右。使用TensorRT能够优化几乎所有主流框架上训练的模型，并且在提高速度的同时只有很小的精度损失。

![tensorrt](https://github.com/dayekuaipao/dayekuaipao.github.io/tree/master/images/tensorrt.png)

TensorRT首先解析已经在主流深度学习框架中（Caffe，英伟达有自己的Caffe分支，叫nvCaffe，支持fp16；TensorFlow，现在TensorRT已经嵌入到TensorFlow中；ONNX，包括Pytorch，Darknet，MXNet，PaddlePaddle等）已经训练好的模型。解析完成后，会生成一个优化之后的运行时引擎。这个引擎可以部署在数据中心乃至嵌入式设备的很多地方。

TensorRT同时支持C++和Python API。其文档参考：https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html。

样例参考：https://docs.nvidia.com/deeplearning/sdk/tensorrt-sample-support-guide/index.html。Github在：https://github.com/NVIDIA/TensorRT。

# 第一步：环境配置：

TensorRT的环境配置有多种形式。可以通过docker，也可以通过debian或者rpm包。还可以通过tar或zip压缩包。这里我们通过压缩包的形式来进行安装。注意使用压缩包安装必须保证依赖项的安装是正确的。包括CUDA和cuDNN等，而且版本号一定要对应。

1. 我们首先到https://developer.nvidia.com/nvidia-tensorrt-download下载对应的tar包。需要注册英伟达开发者账号。进去之后选择版本。每个版本里面又分为GA（Generally Available）和RC（Release Candidate）版本。RC基本上是稳定版本，一般版本号较老，GA是还处于测试中的但是已经基本稳定的版本，版本号较新。两者都可以选择。我们选择GA版本来尝新。

2. 下载完成后，将其解压：

`tar xzvf TensorRT-5.1.x.x.<os>.<arch>-gnu.cuda-x.x.cudnn7.x.tar.gz`

然后将lib目录添加到系统中：

`export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<eg:TensorRT-5.1.x.x/lib>`

3. 安装python包：

`cd TensorRT-5.1.x.x/python`

如果是python2的话：

`sudo pip2 install tensorrt-5.1.x.x-cp27-none-linux_x86_64.whl`

如果是python3的话：

`sudo pip3 install tensorrt-5.1.x.x-cp3x-none-linux_x86_64.whl`

4. 安装uff的python包，这个包主要是为tensorflow提供支持。

`cd TensorRT-5.1.x.x/uff`

如果是python2：

`sudo pip2 install uff-0.6.3-py2.py3-none-any.whl`

如果是python3：

`sudo pip3 install uff-0.6.3-py2.py3-none-any.whl`

使用：

`which convert-to-uff`

如果输出

`/usr/local/bin/convert-to-uff`

表明安装成功。

5. 安装graphsurgeon：

`cd TensorRT-5.1.x.x/graphsurgeon`

如果是python2：

`sudo pip2 install graphsurgeon-0.4.1-py2.py3-none-any.whl`

如果是python3：

`sudo pip3 install graphsurgeon-0.4.1-py2.py3-none-any.whl`

第二步：运行用例：

我们的例子包含3步：导入一个ONNX模型并将其转换为TensorRT网络，使用优化器生成引擎，然后执行推导。我们使用C++，但是也可以用python，API请参考之前提到的链接。我们会用到一下组件：

- ONNX parser：输入ONNX格式的模型，生成TensotRT格式的模型

- Builder：输入TensorRT模型，优化后生成目标平台的引擎

- Engine：获取输入，执行推断和输出推断结构

- Logger：与builder和engine相关的对象，负责错误，警告和其他信息的输出

1. 配置ONNX，下载模型与测试数据：

第一步是从磁盘里导入一个模型然后把他从原来的框架转换为TensorRT模型。我们的例子是导入一个ONNX model zoo里的ResNet50。ONNX是一个多个框架联合的深度学习框架表示标准，现已支持Caffe2，Chainer，CNTK，PaddlePaddle，PyTorch和MXNet等。

我们需要先配置ONNX的环境：

`apt-get update`

`git clone --recursive https://github.com/onnx/onnx.git # pull onnx repository from github`

`cd onnx`

`cmake . # compile and install onnx`

`make install -j12`

下载这个模型和测试数据：

 `wget https://s3.amazonaws.com/onnx-model-zoo/resnet/resnet50v2/resnet50v2.tar.gz `

解压：

`tar xvf resnet50v2.tar.gz`

可以看到里面是三个测试集和一个ONNX模型。测试集是以pb格式封装起来的。所以我们要安装protobuf：

apt install libprotobuf-dev protobuf-compiler 

接下来，我们要开始编写代码来使用TensorRT。

2. 第一份代码：

   ```c++
   #include "cudaWrapper.h"
   
   #include "ioHelper.h"
   
   #include <NvInfer.h>
   
   #include <NvOnnxParser.h>
   
   #include <algorithm>
   
   #include <cassert>
   
   #include <iostream>
   
   #include <memory>
   
   #include <string>
   
   #include <vector>
   
    
   
   using namespace nvinfer1;
   
   using namespace std;
   
   using namespace cudawrapper;
   
    
   
   static Logger gLogger;
   
    
   
   // Number of times we run inference to calculate average time.
   
   constexpr int ITERATIONS = 10;
   
   // Maxmimum absolute tolerance for output tensor comparison against reference.
   
   constexpr double ABS_EPSILON = 0.005;
   
   // Maxmimum relative tolerance for output tensor comparison against reference.
   
   constexpr double REL_EPSILON = 0.05;
   
   // Allow TensorRT to use up to 1GB of GPU memory for tactic selection.
   
   constexpr size_t MAX_WORKSPACE_SIZE = 1ULL << 30; // 1 GB
   
    
   
   ICudaEngine* createCudaEngine(string const& onnxModelPath, int batchSize)
   
   {
   
   ​    unique_ptr<IBuilder, Destroy<IBuilder>> builder{createInferBuilder(gLogger)};
   
   ​    unique_ptr<INetworkDefinition, Destroy<INetworkDefinition>> network{builder->createNetwork()};
   
   ​    unique_ptr<nvonnxparser::IParser, Destroy<nvonnxparser::IParser>> parser{nvonnxparser::createParser(*network, gLogger)};
   
    
   
   ​    if (!parser->parseFromFile(onnxModelPath.c_str(), static_cast<int>(ILogger::Severity::kINFO)))
   
   ​    {
   
   ​        cout << "ERROR: could not parse input engine." << endl;
   
   ​        return nullptr;
   
   ​    }
   
    
   
   ​    // Build TensorRT engine optimized based on for batch size of input data provided.
   
   ​    builder->setMaxBatchSize(batchSize);
   
   ​    // Allow TensorRT to use fp16 mode kernels internally.
   
   ​    // Note that Input and Output tensors will still use 32 bit float type by default.
   
   ​    builder->setFp16Mode(builder->platformHasFastFp16());
   
   ​    builder->setMaxWorkspaceSize(MAX_WORKSPACE_SIZE);
   
    
   
   ​    return builder->buildCudaEngine(*network); // Build and return TensorRT engine.
   
   }
   
    
   
   ICudaEngine* getCudaEngine(string const& onnxModelPath, int batchSize)
   
   {
   
   ​    string enginePath{getBasename(onnxModelPath) + "_batch" + to_string(batchSize) + ".engine"};
   
   ​    ICudaEngine* engine{nullptr};
   
    
   
   ​    string buffer = readBuffer(enginePath);
   
   ​    if (buffer.size())
   
   ​    {
   
   ​        // Try to deserialize engine.
   
   ​        unique_ptr<IRuntime, Destroy<IRuntime>> runtime{createInferRuntime(gLogger)};
   
   ​        engine = runtime->deserializeCudaEngine(buffer.data(), buffer.size(), nullptr);
   
   ​    }
   
    
   
   ​    if (!engine)
   
   ​    {
   
   ​        // Fallback to creating engine from scratch.
   
   ​        engine = createCudaEngine(onnxModelPath, batchSize);
   
    
   
   ​        if (engine)
   
   ​        {
   
   ​            unique_ptr<IHostMemory, Destroy<IHostMemory>> engine_plan{engine->serialize()};
   
   ​            // Try to save engine for future uses.
   
   ​            writeBuffer(engine_plan->data(), engine_plan->size(), enginePath);
   
   ​        }
   
   ​    }
   
   ​    return engine;
   
   }
   
    
   
   static int getBindingInputIndex(IExecutionContext* context)
   
   {
   
   ​    return !context->getEngine().bindingIsInput(0); // 0 (false) if bindingIsInput(0), 1 (true) otherwise
   
   }
   
    
   
   void launchInference(IExecutionContext* context, cudaStream_t stream, vector<float> const& inputTensor, vector<float>& outputTensor, void** bindings, int batchSize)
   
   {
   
   ​    int inputId = getBindingInputIndex(context);
   
    
   
   ​    cudaMemcpyAsync(bindings[inputId], inputTensor.data(), inputTensor.size() * sizeof(float), cudaMemcpyHostToDevice, stream);
   
   ​    context->enqueue(batchSize, bindings, stream, nullptr);
   
   ​    cudaMemcpyAsync(outputTensor.data(), bindings[1 - inputId], outputTensor.size() * sizeof(float), cudaMemcpyDeviceToHost, stream);
   
   }
   
    
   
   void doInference(IExecutionContext* context, cudaStream_t stream, vector<float> const& inputTensor, vector<float>& outputTensor, void** bindings, int batchSize)
   
   {
   
   ​    CudaEvent start;
   
   ​    CudaEvent end;
   
   ​    double totalTime = 0.0;
   
    
   
   ​    for (int i = 0; i < ITERATIONS; ++i)
   
   ​    {
   
   ​        float elapsedTime;
   
    
   
   ​        // Measure time it takes to copy input to GPU, run inference and move output back to CPU.
   
   ​        cudaEventRecord(start, stream);
   
   ​        launchInference(context, stream, inputTensor, outputTensor, bindings, batchSize);
   
   ​        cudaEventRecord(end, stream);
   
    
   
   ​        // Wait until the work is finished.
   
   ​        cudaStreamSynchronize(stream);
   
   ​        cudaEventElapsedTime(&elapsedTime, start, end);
   
    
   
   ​        totalTime += elapsedTime;
   
   ​    }
   
    
   
   ​    cout << "Inference batch size " << batchSize << " average over " << ITERATIONS << " runs is " << totalTime / ITERATIONS << "ms" << endl;
   
   }
   
    
   
   void softmax(vector<float>& tensor, int batchSize)
   
   {
   
   ​    size_t batchElements = tensor.size() / batchSize;
   
    
   
   ​    for (int i = 0; i < batchSize; ++i)
   
   ​    {
   
   ​        float* batchVector = &tensor[i * batchElements];
   
   ​        double maxValue = *max_element(batchVector, batchVector + batchElements);
   
   ​        double expSum = accumulate(batchVector, batchVector + batchElements, 0.0, [=](double acc, float value) { return acc + exp(value - maxValue); });
   
    
   
   ​        transform(batchVector, batchVector + batchElements, batchVector, [=](float input) { return static_cast<float>(std::exp(input - maxValue) / expSum); });
   
   ​    }
   
   }
   
    
   
   void verifyOutput(vector<float> const& outputTensor, vector<float> const& referenceTensor)
   
   {
   
   ​    for (size_t i = 0; i < referenceTensor.size(); ++i)
   
   ​    {
   
   ​        double reference = static_cast<double>(referenceTensor[i]);
   
   ​        // Check absolute and relative tolerance.
   
   ​        if (abs(outputTensor[i] - reference) > max(abs(reference) * REL_EPSILON, ABS_EPSILON))
   
   ​        {
   
   ​            cout << "ERROR: mismatch at position " << i;
   
   ​            cout << " expected " << reference << ", but was " << outputTensor[i] << endl;
   
   ​            return;
   
   ​        }
   
   ​    }
   
    
   
   ​    cout << "OK" << endl;
   
   }
   
    
   
   int main(int argc, char* argv[])
   
   {
   
   ​    // Declaring cuda engine.
   
   ​    unique_ptr<ICudaEngine, Destroy<ICudaEngine>> engine{nullptr};
   
   ​    // Declaring execution context.
   
   ​    unique_ptr<IExecutionContext, Destroy<IExecutionContext>> context{nullptr};
   
   ​    vector<float> inputTensor;
   
   ​    vector<float> outputTensor;
   
   ​    vector<float> referenceTensor;
   
   ​    void* bindings[2]{0};
   
   ​    vector<string> inputFiles;
   
   ​    CudaStream stream;
   
    
   
   ​    if (argc < 3)
   
   ​    {
   
   ​        cout << "usage: " << argv[0] << " <path_to_model.onnx> (1.. <path_to_input.pb>)" << endl;
   
   ​        return 1;
   
   ​    }
   
    
   
   ​    string onnxModelPath(argv[1]);
   
   ​    
   
   ​    inputFiles.push_back(string{argv[2]});
   
    
   
   ​    int batchSize = inputFiles.size();
   
    
   
   ​    // Create Cuda Engine.
   
   ​    engine.reset(getCudaEngine(onnxModelPath, batchSize));
   
   ​    if (!engine)
   
   ​        return 1;
   
    
   
   ​    // Assume networks takes exactly 1 input tensor and outputs 1 tensor.
   
   ​    assert(engine->getNbBindings() == 2);
   
   ​    assert(engine->bindingIsInput(0) ^ engine->bindingIsInput(1));
   
    
   
   ​    for (int i = 0; i < engine->getNbBindings(); ++i)
   
   ​    {
   
   ​        Dims dims{engine->getBindingDimensions(i)};
   
   ​        size_t size = accumulate(dims.d, dims.d + dims.nbDims, batchSize, multiplies<size_t>());
   
   ​        // Create CUDA buffer for Tensor.
   
   ​        cudaMalloc(&bindings[i], size * sizeof(float));
   
    
   
   ​        // Resize CPU buffers to fit Tensor.
   
   ​        if (engine->bindingIsInput(i))
   
   ​            inputTensor.resize(size);
   
   ​        else
   
   ​            outputTensor.resize(size);
   
   ​    }
   
    
   
   ​    // Read input tensor from ONNX file.
   
   ​    if (readTensor(inputFiles, inputTensor) != inputTensor.size())
   
   ​    {
   
   ​        cout << "Couldn't read input Tensor" << endl;
   
   ​        return 1;
   
   ​    }
   
    
   
   ​    // Create Execution Context.
   
   ​    context.reset(engine->createExecutionContext());
   
    
   
   ​    doInference(context.get(), stream, inputTensor, outputTensor, bindings, batchSize);
   
    
   
   ​    vector<string> referenceFiles;
   
   ​    for (string path : inputFiles)
   
   ​        referenceFiles.push_back(path.replace(path.rfind("input"), 5, "output"));
   
   ​    // Try to read and compare against reference tensor from protobuf file.
   
   ​    referenceTensor.resize(outputTensor.size());
   
   ​    if (readTensor(referenceFiles, referenceTensor) != referenceTensor.size())
   
   ​    {
   
   ​        cout << "Couldn't read reference Tensor" << endl;
   
   ​        return 1;
   
   ​    }
   
    
   
   ​    // Apply a softmax on the CPU to create a normalized distribution suitable for measuring relative error in probabilities.
   
   ​    softmax(outputTensor, batchSize);
   
   ​    softmax(referenceTensor, batchSize);
   
    
   
   ​    verifyOutput(outputTensor, referenceTensor);
   
    
   
   ​    for (void* ptr : bindings)
   
   ​        cudaFree(ptr);
   
    
   
   ​    return 0;
   
   }
   ```



我们来对其中的关键部分进行分析：

首先看main函数，我们要先声明一个CUDA引擎来保存网络定义和训练参数。这个引擎随后由createCudaEngine这个函数生成。

```c++
// Declare CUDA engine

unique_ptr<ICudaEngine, Destroy<ICudaEngine>> engine{nullptr};

...

// Create CUDA Engine

engine.reset(createCudaEngine(onnxModelPath));
```



createCudaEngine函数会解析ONNX模型，并将其存在network中，之后builder又将network转换为engine：

```c++
ICudaEngine* createCudaEngine(string const& onnxModelPath)

 

{

   unique_ptr<IBuilder, Destroy<IBuilder>> builder{createInferBuilder(gLogger)};

   unique_ptr<INetworkDefinition, Destroy<INetworkDefinition>> network{builder->createNetwork()};

   unique_ptr<nvonnxparser::IParser, Destroy<nvonnxparser::IParser>> parser{nvonnxparser::createParser(*network, gLogger)};

   if (!parser->parseFromFile(onnxModelPath.c_str(), static_cast<int>(ILogger::Severity::kINFO)))

​      {

​         cout << "ERROR: could not parse input engine." << endl;

​         return nullptr;

​      }

   return builder->buildCudaEngine(*network); // build and return TensorRT engine

}
```



一旦engine被生成，我们需要创建一个execution context来存放推断时产生的中间激活值：

```c++
// declaring execution context

unique_ptr<IExecutionContext, Destroy<IExecutionContext>> context{nullptr};

 

...

 

// create execution context

context.reset(engine->createExecutionContext());
```



函数launchInference会将推断请求异步地放到GPU上。步骤是：launchInference将输入从host（CPU）拷贝到device（GPU）上，然后推断会在enqueue函数中执行，最后结果被异步地拷贝回来。这个例子使用CUDA流来管理GPU上的异步工作。异步执行可以最大程度地利用GPU，提高执行效率。host与device之间的数据拷贝是用cudaMemcpyAsync实现的。

我们可以在launchInference之后使用cudaStreamSynchronize来确保同步，即GPU运算完成后，结果才能被访问。这些运算结果会被ICudaEngine使用。最后，程序会比较TensorRT的输出结果和参考结果。

这个应用程序只会有一个输入，也就是说，batchsize为1。但我们实际使用时通常会多个batchsize来并行运算。例如，在V100或者T4上，使用32的倍数的batchsize效率会很高。通过下面的命令可以将图片传入模型，图片（.pb文件）的数目（即batchsize）是命令的参数之一，使用test_data_set_*可以将所有文件夹下的input_0.pb文件输入。当前有三个文件夹，所以batchsize是3。

由于我们现在需要处理多张图片了，我们要对我们的实现进行一些修改。

将

`inputFiles.push_back(string{argv[2]});`

改为：

``` 
for (int i = 2; i < argc; ++i)
inputFiles.push_back(string{argv[i]});
```



接下来我们要使用setMaxBatchSize这个函数来指定最大的batchsize，这样builder就会生成一个能够针对这个batchsize的最优化算法。虽然这个模型不能输入比这个更大的batchsize，但是可以输入更小的。对于maxbatchsize的值的选择，取决于每次输入的图片的数目。一个常用的策略是构建很多不同batchsize的模型，然后运行时选择最优的那个。模型情况下。batchsize为1.

```c++
// Build TensorRT engine optimized based on batch size of input data provided

builder->setMaxBatchSize(batchSize);
```

接下来我们将考虑如何评价模型的性能，主要是两部分，一是延迟，二是吞吐量，cuda为我们提供了一些API让我们来计算延迟，如：

```c++
// Number of times we run inference to calculate average time

constexpr int ITERATIONS = 10;

...

void doInference(IExecutionContext* context, cudaStream_t stream, vector const& inputTensor, vector& outputTensor, void** bindings, int batchSize)

{

​    CudaEvent start;

​    CudaEvent end;

​    double totalTime = 0.0;

 

​    for (int i = 0; i < ITERATIONS; ++i)

​    {

​        float elapsedTime;

 

​        // Measure time it takes to copy input to GPU, run inference and move output back to CPU

​        cudaEventRecord(start, stream);

​        launchInference(context, stream, inputTensor, outputTensor, bindings, batchSize);

​        cudaEventRecord(end, stream);

 

​        // wait until the work is finished

​        cudaStreamSynchronize(stream);

​        cudaEventElapsedTime(&elapsedTime, start, end);

 

​        totalTime += elapsedTime;

​    }

 

​    cout << "Inference batch size " << batchSize << " average over " << ITERATIONS << " runs is " << totalTime / ITERATIONS << "ms" << endl;

}
```

使用混合精度：

我们可以使用fp16或者int8来加速推断，使用下面的函数：

`builder->setFp16Mode(builder->platformHasFastFp16());`

我们还可以设置TensorRT能占用的最大显存：

```c++


​```

// Allow TensorRT to use up to 1GB of GPU memory for tactic selection

constexpr size_t MAX_WORKSPACE_SIZE = 1ULL << 30; // 1 GB worked well for this sample

...

// set builder flag 

builder->setMaxWorkspaceSize(MAX_WORKSPACE_SIZE);
```

当引擎建立之后，我们可以复用这个引擎，即将引擎序列化后存入硬盘中。在使用时，我们将模型从硬盘中读取，然后反序列化。如：

```c++
ICudaEngine* getCudaEngine(string const& onnxModelPath, int batchSize)

{

​    string enginePath{getBasename(onnxModelPath) + "_batch" + to_string(batchSize) + ".engine"};

​    ICudaEngine* engine{nullptr};

 

​    string buffer = readBuffer(enginePath);

​    if (buffer.size())

​    {

​        // try to deserialize engine

​        unique_ptr<IRuntime, Destroy> runtime{createInferRuntime(gLogger)};

​        engine = runtime->deserializeCudaEngine(buffer.data(), buffer.size(), nullptr);

​    }

 

​    if (!engine)

​    {

​        // Fallback to creating engine from scratch

​        engine = createCudaEngine(onnxModelPath, batchSize);

 

​        if (engine)

​        {

​            unique_ptr<IHostMemory, Destroy> engine_plan{engine->serialize()};

​            // try to save engine for future uses

​            writeBuffer(engine_plan->data(), engine_plan->size(), enginePath);

​        }

​    }

​    return engine;

}
```

以上就是一个使用TensorRT的例子。